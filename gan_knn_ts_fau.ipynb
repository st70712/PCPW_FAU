{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import h5py\n",
    "from pylab import *\n",
    "from libs.utils import weight_variable, bias_variable\n",
    "from libs.readfile_fau import *\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import SGD\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data：(9959, 256) , (9959, 5) , [881, 2093, 5590, 674, 721]\n",
      "Test data：(8257, 256) , (8257, 5) , [611, 1508, 5377, 215, 546]\n"
     ]
    }
   ],
   "source": [
    "#load the train and test data\n",
    "filename_train = './data/FAU_train_256.arff'\n",
    "filename_test =  './data/FAU_test_256.arff'\n",
    "train_data, train_label, train_size = readFile(filename_train)\n",
    "test_data, test_label, test_size = readFile(filename_test)\n",
    "print(\"Train data：{0} , {1} , {2}\".format(train_data.shape, train_label.shape, train_size))\n",
    "print(\"Test data：{0} , {1} , {2}\".format(test_data.shape, test_label.shape, test_size))\n",
    "fau_a,fau_e,fau_n,fau_p,fau_r = separa_fau(train_data, train_label)\n",
    "\n",
    "combine = [fau_a,fau_e,fau_n,fau_p,fau_r]\n",
    "emotion = ['Anger', 'Emphatic', 'Neutral', 'Positive', 'Rest']\n",
    "fake_emotion_txt = ['fake_a.txt', 'fake_e.txt', 'fake_n.txt', 'fake_p.txt', 'fake_r.txt']\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "input_height = 16\n",
    "input_width = 16\n",
    "\n",
    "d_dim = 256\n",
    "g_dim = 100\n",
    "\n",
    "gfc_dim = 1024\n",
    "dfc_dim = 1024\n",
    "\n",
    "g_filter_dim = 64\n",
    "d_filter_dim = 64\n",
    "color_dim = 1\n",
    "\n",
    "learning_rate = 0.0002\n",
    "beta1 = 0.5\n",
    "epsilon = 1e-5\n",
    "fake_num = 2000\n",
    "\n",
    "x_d = tf.placeholder(tf.float32, shape = [None, d_dim])\n",
    "x_g = tf.placeholder(tf.float32, shape = [None, g_dim])\n",
    "\n",
    "t_vars = tf.trainable_variables()\n",
    "var_d = [var for var in t_vars if 'd_' in var.name]\n",
    "var_g = [var for var in t_vars if 'g_' in var.name]\n",
    "\n",
    "# leaky relu activation function\n",
    "def lrelu(x, leak=0.2, name=\"lrelu\"):\n",
    "      return tf.maximum(x, leak*x)\n",
    "    \n",
    "# batch normalization\n",
    "def batch_norm(x, train, name='batch_norm'):\n",
    "    return tf.contrib.layers.batch_norm( x, decay=0.9, updates_collections=None,\n",
    "                                                                  epsilon=epsilon,\n",
    "                                                                  scale=True,\n",
    "                                                                  is_training=train,\n",
    "                                                                  scope=name)\n",
    "\n",
    "def conv2d(input_, output_dim,   k_h=5, k_w=5, d_h=2, d_w=2, stddev=0.02, name='conv2d'):\n",
    "    with tf.variable_scope(name):\n",
    "        kernel = tf.get_variable('kernel', [k_h, k_w, input_.get_shape()[-1], output_dim],initializer=tf.truncated_normal_initializer(stddev=stddev))\n",
    "        conv = tf.nn.conv2d(input_, kernel, strides=[1, d_h, d_w, 1], padding='SAME')\n",
    "\n",
    "        bias = tf.get_variable('bias', [output_dim], initializer=tf.constant_initializer(0.0))\n",
    "        conv = tf.reshape(tf.nn.bias_add(conv, bias), conv.get_shape())\n",
    "\n",
    "        return conv\n",
    "\n",
    "def deconv2d(input_, output_shape,k_h=5, k_w=5, d_h=2, d_w=2, stddev=0.02,name=\"deconv2d\", with_w=False):\n",
    "    with tf.variable_scope(name):\n",
    "        # filter : [height, width, output_channels, in_channels]\n",
    "        kernel = tf.get_variable('kernel', [k_h, k_w, output_shape[-1], input_.get_shape()[-1]],initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "        deconv = tf.nn.conv2d_transpose(input_, kernel, output_shape=output_shape,strides=[1, d_h, d_w, 1])\n",
    "\n",
    "        bias = tf.get_variable('biases', [output_shape[-1]], initializer=tf.constant_initializer(0.0))\n",
    "        deconv = tf.reshape(tf.nn.bias_add(deconv, bias), deconv.get_shape())\n",
    "\n",
    "        if with_w:\n",
    "              return deconv, kernel, bias\n",
    "        else:\n",
    "              return deconv\n",
    "        \n",
    "def linear(input_, output_size, scope=None, stddev=0.02, bias_start=0.0, with_w=False):\n",
    "    input_ = tf.convert_to_tensor(input_, dtype=tf.float32)\n",
    "    shape = input_.get_shape().as_list()\n",
    "\n",
    "    with tf.variable_scope(scope or \"Linear\"):\n",
    "        matrix = tf.get_variable(\"Matrix\", [shape[1], output_size], tf.float32, tf.random_normal_initializer(stddev=stddev))\n",
    "        bias = tf.get_variable(\"bias\", [output_size],initializer=tf.constant_initializer(bias_start))\n",
    "        if with_w:\n",
    "              return tf.matmul(input_, matrix) + bias, matrix, bias\n",
    "        else:\n",
    "              return tf.matmul(input_, matrix) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GAN block ###\n",
    "def sample_z(sample_num, z_dim):\n",
    "    return  np.random.normal(0.0, 1.0, size=[sample_num, z_dim]) # use gaussian distribution to generate noise sample (mean=0, stddev=1)\n",
    "\n",
    "def generator(z, is_training=True):\n",
    "    # use ReLU as activation function except output layer\n",
    "    # project z and reshape, 4x4x512\n",
    "    z_linear, h0_w, h0_b = linear(z, 4*4*g_filter_dim*8, 'g_h0_linear', with_w=True)\n",
    "    h0 = tf.reshape(z_linear, [-1, 4, 4, g_filter_dim*8])\n",
    "    h0 = lrelu(batch_norm(h0, is_training, name='g_bn0'))\n",
    "    # first deconvolution, 8x8x256\n",
    "    h1, h1_w, h1_b = deconv2d(h0, [batch_size, 8, 8, g_filter_dim*4], name='g_h1_deconv', with_w=True)\n",
    "    h1 = lrelu(batch_norm(h1, is_training, name='g_bn1'))\n",
    "    # second deconvolution, 16x16x128\n",
    "    h2, h2_w, h2_b = deconv2d(h1, [batch_size, 16, 16, g_filter_dim*2], name='g_h2_deconv', with_w=True)\n",
    "    h2 = lrelu(batch_norm(h2, is_training, name='g_bn2'))\n",
    "    # third deconvolution, 16x16x1\n",
    "    h3, h3_w, h3_b = deconv2d(h2, [batch_size, 16, 16, color_dim], name='g_h3_deconv', with_w=True)\n",
    "    # output layer use hyperbolic tangent (tanh)\n",
    "    return tf.nn.tanh(h3)\n",
    "\n",
    "def discriminator(x):\n",
    "     # use leaky relu as activation function except output layer\n",
    "    print(x.shape)\n",
    "    feature = tf.reshape(x, [-1, 16,16, 1])\n",
    "    # first convolution, 8x8x128\n",
    "    h0 = lrelu(conv2d(feature, d_filter_dim*2, name='d_h0_conv'))\n",
    "    # second convolution, 4x4x256\n",
    "    h1 = lrelu(batch_norm(conv2d(h0, d_filter_dim*4, name='d_h1_conv'), name='d_bn1'))\n",
    "    # third convolution, 2x2x512\n",
    "    h2 = lrelu(batch_norm(conv2d(h1, d_filter_dim*8, name='d_h2_conv'), name='d_bn2'))\n",
    "    # fully-connected layer, 1x1x1024\n",
    "    h3 = linear(tf.reshape(h2, [batch_size, -1]), dfc_dim, 'd_h2_linear')\n",
    "    # ouptut layer use sigmoid as activation function\n",
    "    return tf.nn.sigmoid(h3), h3\n",
    "\n",
    "def plot(num, samples):\n",
    "    samples = np.concatenate((samples,np.zeros((samples.shape[0],16))),axis=1)\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(20, 20), cmap = 'gray')\n",
    "    plt.savefig('./pic/figure %d .png'%(num))\n",
    "    #plt.show()\n",
    "    \n",
    "def train_gan(emotion_class, emotion_name):\n",
    "    gan_train_data = emotion_class # set the emotion class\n",
    "  \n",
    "    G_sample = generator(x_g)\n",
    "    D_real, D_real_logits = discriminator(x_d)\n",
    "    D_fake, D_fake_logits_ = discriminator(G_sample)\n",
    "    # define the loss function\n",
    "    d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(D_real_logits, tf.ones_like(D_real)))\n",
    "    d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(D_fake_logits_, tf.zeros_like(D_fake)))\n",
    "    g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(D_fake_logits_, tf.ones_like(D_fake)))\n",
    "    d_loss = d_loss_real + d_loss_fake\n",
    "    #d_loss = -tf.reduce_mean(tf.log(d_real) + tf.log(1. - d_fake))\n",
    "    #g_loss = -tf.reduce_mean(tf.log(d_fake))\n",
    "    \n",
    "    # update discriminator\n",
    "    d_optimizer = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(d_loss, var_list = var_d)\n",
    "\n",
    "    # update generator\n",
    "    g_optimizer = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(g_loss, var_list = var_g)\n",
    "    \n",
    "    num = 0\n",
    "    global_step = 30001\n",
    "    sess = tf.InteractiveSession()\n",
    "    tf.global_variables_initializer().run()\n",
    "    saver = tf.train.Saver()\n",
    "    for step in range(global_step):\n",
    "        offset = (step * batch_size) % (gan_train_data.shape[0] - batch_size)\n",
    "        batch_x = gan_train_data[offset:(offset + batch_size), :]\n",
    "        _ , d_loss_train = sess.run([d_optimizer, d_loss], feed_dict = {x_d: batch_x, x_g: sample_z(batch_size, g_dim)})\n",
    "        _ , g_loss_train = sess.run([g_optimizer, g_loss], feed_dict = {x_g: sample_z(batch_size, g_dim)})\n",
    "        if step % 2000 == 0:\n",
    "            print(\"Step %d, discriminator loss %.5f , generator loss %.5f\" % (step, d_loss_train, g_loss_train))\n",
    "        if step % 1000 == 0: \n",
    "            g_sample_plot = g_sample.eval(feed_dict = {x_g: sample_z(16, g_dim)})\n",
    "            plot(num,g_sample_plot)\n",
    "            path_name = './gan_model/' + emotion_name\n",
    "            save_path = saver.save(sess, path_name + '/model%d.ckpt'% (num))\n",
    "            num+=1\n",
    "            \n",
    "def generate_fake_data():\n",
    "    fake_emotion_list = [[], [], [], [], []]\n",
    "    for i in range(5):\n",
    "        fake_emotion_model = './gan_model/' + emotion[i] + '/model30.ckpt'\n",
    "        with tf.Session() as sess:\n",
    "            saver = tf.train.Saver()\n",
    "            saver.restore(sess,  fake_emotion_model)\n",
    "            g_sample = generator(x_g, False)\n",
    "            new_fau_sample = g_sample.eval(feed_dict = {x_g: sample_z(fake_num, g_dim)})\n",
    "           #plot(i, new_fau_sample)\\n\",\n",
    "            fake_emotion_list[i] = new_fau_sample\n",
    "\n",
    "    fake_a = fake_emotion_list[0]\n",
    "    fake_e = fake_emotion_list[1]\n",
    "    fake_n = fake_emotion_list[2]\n",
    "    fake_p = fake_emotion_list[3]\n",
    "    fake_r = fake_emotion_list[4]\n",
    "    # save the fake data\n",
    "    np.savetxt(\"./gan_data/Anger/fake_a.txt\", fake_a)\n",
    "    np.savetxt(\"./gan_data/Emphatic/fake_e.txt\", fake_e)\n",
    "    np.savetxt(\"./gan_data/Neutral/fake_n.txt\", fake_n)\n",
    "    np.savetxt(\"./gan_data/Positive/fake_p.txt\", fake_p)\n",
    "    np.savetxt(\"./gan_data/Rest/fake_r.txt\", fake_r)\n",
    "    print ('\\nEnd of generating.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### KNN block ###\n",
    "#Euclidean distance\n",
    "def euclidean(train, test, train_num):\n",
    "    dist=[]\n",
    "    for i in range(train_num):# number of training data: i\n",
    "        dist_temp=[math.sqrt(sum(pow(train[i,:] - test, 2)))]\n",
    "        dist.append(dist_temp)\n",
    "    return dist\n",
    "\n",
    "# Manhattan distance: |x1-x2|+|y1-y2|\n",
    "def manhattan(train, test, train_num):\n",
    "    dist=[]\n",
    "    for i in range(train_num):# number of training data: i\n",
    "        dist_temp=[(sum(abs(train[i,:] - test)))]\n",
    "        dist.append(dist_temp)\n",
    "    return dist\n",
    "\n",
    "#Chebyshev distance:  max(|x2-x1|, |y2-y1|)\n",
    "def chebyshev(train, test, train_num):\n",
    "    dist=[]\n",
    "    for i in range(train_num):# number of training data: i\n",
    "        dist_temp=[np.max(abs((train[i,:] - test)))]\n",
    "        dist.append(dist_temp)\n",
    "    return dist\n",
    "\n",
    "def knn(target_data):\n",
    "    kvalue = 1 # set the k value\n",
    "    result=[0,0,0,0,0]\n",
    "    a_weight=1.0/881.0\n",
    "    e_weight=1.0/2093.0\n",
    "    n_weight=1.0/5590.0\n",
    "    p_weight=1.0/674.0\n",
    "    r_weight=1.0/721.0\n",
    "    dist_choice = 'eu'# distance type (eu , manha , chebyshev)\n",
    "    if(dist_choice == 'eu'):\n",
    "        dist = euclidean(train_data, target_data, train_data.shape[0])\n",
    "    if(dist_choice == 'manha'):\n",
    "        dist = manhattan(train_data, target_data, train_data.shape[0])\n",
    "    if(dist_choice == 'chebyshev'):\n",
    "        dist = chebyshev(train_data, target_data, train_data.shape[0])\n",
    "\n",
    "    dist_label = np.concatenate((train_label, dist), axis=1) # concatenate distance with the corresponding label\n",
    "    sorted_distances = sorted(dist_label, key = lambda x : x[1]) # sort by the distance\n",
    "    sorted_distances=np.asarray(sorted_distances) # convert into an array\n",
    "    \n",
    "    for j in range(kvalue): # record the knn result\n",
    "        m = np.argmax(sorted_distances[j][0:5])\n",
    "        if  m == 0: #Anger\n",
    "            result[0]+=a_weight\n",
    "        elif m == 1: #Emphatic\n",
    "            result[1]+=e_weight\n",
    "        elif m == 2: #Neutral\n",
    "            result[2]+=n_weight\n",
    "        elif m == 3: #Positive\n",
    "            result[3]+=p_weight\n",
    "        elif m == 4: #Rest\n",
    "            result[4]+=r_weight\n",
    "            \n",
    "    temp = np.argmax(result)\n",
    "    if temp == 0: #Anger\n",
    "        return [[1,0,0,0,0]]\n",
    "    elif temp == 1: #Emphatic\n",
    "        return  [[0,1,0,0,0]]\n",
    "    elif temp == 2: #Neutral\n",
    "        return [[0,0,1,0,0]]\n",
    "    elif temp == 3: #Positive\n",
    "        return [[0,0,0,1,0]]\n",
    "    elif temp == 4: #Rest\n",
    "        return [[0,0,0,0,1]]\n",
    "\n",
    "def nearest_img(target,image,label):\n",
    "    nearest=[]\n",
    "    for i in range(target.shape[0]):\n",
    "        dist=[]\n",
    "        for j in range(image.shape[0]-1):\n",
    "            dist_temp=np.sum(np.absolute(target[i]-image[j]))\n",
    "            dist.append(dist_temp)\n",
    "        dist=np.asarray(dist)\n",
    "        nearest.append(label[np.argmin(dist)])\n",
    "    nearest=np.asarray(nearest)\n",
    "    return nearest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### teacher-student model block ###\n",
    "def _to_tensor(x, dtype):\n",
    "    x = tf.convert_to_tensor(x)\n",
    "    if x.dtype != dtype:\n",
    "        x = tf.cast(x, dtype)\n",
    "    return x\n",
    "\n",
    "def weighted_loss(r):\n",
    "    \n",
    "    def new_loss(y_true, y_pred):\n",
    "        _EPSILON = 10e-8\n",
    "        y_pred /= tf.reduce_sum(y_pred, reduction_indices=len(y_pred.get_shape()) - 1, keep_dims=True)\n",
    "        # manual computation of crossentropy\n",
    "        epsilon = _to_tensor(_EPSILON, y_pred.dtype.base_dtype)\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "        \n",
    "        #y_true = tf.cast(y_true, tf.float64)\n",
    "        rr = tf.cast(r, tf.float32)\n",
    "        new_R = tf.matmul(rr, y_true, transpose_b=True)\n",
    "        \n",
    "        return - new_R * tf.reduce_sum(y_true * tf.log(y_pred), reduction_indices=len(y_pred.get_shape()) - 1)\n",
    "        #return r * K.categorical_crossentropy(y_true, y_pred)\n",
    "\n",
    "    return new_loss\n",
    "\n",
    "def tea_stu_model(train_data, train_label, is_training):\n",
    "    filename_test =  './data/FAU_test_256.arff'\n",
    "    test_data, test_label, test_size = readFile(filename_test)\n",
    "    test_data = normalize(test_data)\n",
    "    input_dim = 256\n",
    "    learning_rate = 0.01\n",
    "    momentum = 0.5\n",
    "    decay = 0.0005\n",
    "    batch_size = 100\n",
    "    epoch = 600\n",
    "    # load the existing model and predict \n",
    "    if is_training == False:\n",
    "        model = Sequential()\n",
    "        model.add(Dense(30, activation='sigmoid', input_shape=(input_dim,)))\n",
    "        model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "        model.load_weights(\"./model/model.h5\")\n",
    "        #model.summary()\n",
    "        result = model.predict(train_data)\n",
    "        use_data = result\n",
    "\n",
    "        for i, (Target, Label) in enumerate( zip(test_label,use_data) ) :\n",
    "            m = np.max(Label)# recognition result (max class)\n",
    "            for j, value in enumerate(Label) : \n",
    "                if value == m :\n",
    "                    if j == 0: # Anger\n",
    "                        return [[1,0,0,0,0]]\n",
    "                    if j == 1: # Emphatic\n",
    "                        return [[0,1,0,0,0]]\n",
    "                    if j == 2: # Neutral\n",
    "                        return [[0,0,1,0,0]]\n",
    "                    if j == 3: # Positive\n",
    "                        return [[0,0,0,1,0]]\n",
    "                    if j == 4: # Rest\n",
    "                        return [[0,0,0,0,1]]\n",
    "\n",
    "    # retraining the ts-model and predict\n",
    "    else:\n",
    "        filename_test =  './data/CS_Ftest_nor.arff'\n",
    "        test_data, test_label, test_size = readFile(filename_test)\n",
    "        print(\"Train data：{0} , {1}\".format(train_data.shape, train_label.shape))\n",
    "        print(\"Test data：{0} , {1}\".format(test_data.shape, test_label.shape))\n",
    "        weight_data = []\n",
    "        weight_data.append([1.1,0.5,0.2,1.5,1.4])\n",
    "        weight_data = np.asarray(weight_data)\n",
    "\n",
    "        # use train data to generate teacher label\n",
    "        model = Sequential()\n",
    "        model.add(Dense(30, activation='sigmoid', input_shape=(input_dim,)))\n",
    "        model.add(Dense(5, activation='softmax'))\n",
    "        model.load_weights(\"./model/ts_46_model.h5\")# load the existing model\n",
    "        model.summary()\n",
    "        \n",
    "        sgd = SGD(lr=learning_rate, momentum=momentum, decay=decay)\n",
    "        model.compile(loss=weighted_loss(r=weight_data), optimizer=sgd)\n",
    "        history = model.fit(train_data, train_label,batch_size=100, epochs=600,verbose=2)    \n",
    "        result = model.predict(train_data)\n",
    "        np.save(\"teacher_label\",result) # save the teacher label\n",
    "        \n",
    "        # use teacher label to retrain model and predict\n",
    "        teacher_label = np.load(\"teacher_label.npy\")\n",
    "        train_label = teacher_label # use teacher_label\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Dense(30, activation='sigmoid', input_shape=(input_dim,)))\n",
    "        model.add(Dense(5, activation='softmax'))\n",
    "        #model.load_weights(\"./model/ts_46_model.h5\")# load the existing model\n",
    "        model.summary()\n",
    "        sgd = SGD(lr=learning_rate, momentum=momentum, decay=decay)\n",
    "        model.compile(loss=weighted_loss(r=weight_data), optimizer=sgd)\n",
    "        history = model.fit(train_data, train_label, batch_size=batch_size, epochs=epoch, verbose=2)    \n",
    "        result = model.predict(test_data)\n",
    "        \n",
    "        AC_A = 0.0\n",
    "        AC_E = 0.0\n",
    "        AC_N = 0.0\n",
    "        AC_P = 0.0\n",
    "        AC_R = 0.0\n",
    "        AC_WA = 0.0\n",
    "        AC_UA = 0.0\n",
    "        right=0# correct num\n",
    "        matrix = np.zeros((5, 5))\n",
    "\n",
    "        use_data = result\n",
    "\n",
    "        for i, (Target, Label) in enumerate( zip(test_label,use_data) ) :  ### target:答案 label:辨識結果\n",
    "            m = np.max(Label)#辨識結果（最大類別）\n",
    "            for j, value in enumerate(Label) :  ### 分到哪類\n",
    "                if value == m :\n",
    "                    for k, num in enumerate(Target) :  ### 正確為哪類\n",
    "                        if num == 1 :\n",
    "                            matrix[k][j] += 1\n",
    "                            break  # end of for k\n",
    "                    break  # end of for j\n",
    "\n",
    "        AC_A = matrix[0][0] / test_size[0]\n",
    "        AC_E = matrix[1][1] / test_size[1]\n",
    "        AC_N = matrix[2][2] / test_size[2]\n",
    "        AC_P = matrix[3][3] / test_size[3]\n",
    "        AC_R = matrix[4][4] / test_size[4]\n",
    "        print(\"\\n-------------實驗結果-------------\")\n",
    "        print ('A: {0} , E: {1} , N: {2} , P:{3} , R: {4}'.format(test_size[0],test_size[1],test_size[2],test_size[3],test_size[4],))\n",
    "        print(\"Total {}:\".format(len(use_data)))\n",
    "        AC_UA = (AC_A + AC_E + AC_N + AC_P + AC_R) / 5\n",
    "        AC_WA = (matrix[0][0] + matrix[1][1] + matrix[2][2] + matrix[3][3] + matrix[4][4]) / len(use_data)\n",
    "        right = matrix[0][0] + matrix[1][1] + matrix[2][2] + matrix[3][3] + matrix[4][4]\n",
    "        print('      A      E      N      P      R')\n",
    "        print('A  %4d   %4d   %4d   %4d   %4d    ' % (matrix[0][0], matrix[0][1], matrix[0][2], matrix[0][3], matrix[0][4]))\n",
    "        print('E  %4d   %4d   %4d   %4d   %4d    ' % (matrix[1][0], matrix[1][1], matrix[1][2], matrix[1][3], matrix[1][4]))\n",
    "        print('N  %4d   %4d   %4d   %4d   %4d    ' % (matrix[2][0], matrix[2][1], matrix[2][2], matrix[2][3], matrix[2][4]))\n",
    "        print('P  %4d   %4d   %4d   %4d   %4d    ' % (matrix[3][0], matrix[3][1], matrix[3][2], matrix[3][3], matrix[3][4]))\n",
    "        print('R  %4d   %4d   %4d   %4d   %4d    ' % (matrix[4][0], matrix[4][1], matrix[4][2], matrix[4][3], matrix[4][4]))\n",
    "        print('\\nA: %f    E: %f     N: %f     P: %f     R: %f\\n' % (AC_A*100, AC_E*100, AC_N*100, AC_P*100, AC_R*100))\n",
    "        print('Correct: {}'.format(right))\n",
    "        print(\"Correct rate: {}\".format(AC_UA*100))\n",
    "        print(\"=====================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: Anger training procedure\n",
      "(?, 256)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot convert a partially known TensorShape to a Tensor: (?, 8, 8, 128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-1f13235a8d95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0memo_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memo_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memotion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Class: %s training procedure'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0memo_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain_gan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memo_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memo_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'\\nEnd of training\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-b09d50f37a4e>\u001b[0m in \u001b[0;36mtrain_gan\u001b[0;34m(emotion_class, emotion_name)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mG_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_g\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mD_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_real_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0mD_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_fake_logits_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# define the loss function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-b09d50f37a4e>\u001b[0m in \u001b[0;36mdiscriminator\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# first convolution, 8x8x128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mh0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_filter_dim\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'd_h0_conv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;31m# second convolution, 4x4x256\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mh1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_filter_dim\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'd_h1_conv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'd_bn1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-3f5cc824736b>\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input_, output_dim, k_h, k_w, d_h, d_w, stddev, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bias'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0moutput_dim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mconv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.pyc\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m   2628\u001b[0m   \"\"\"\n\u001b[1;32m   2629\u001b[0m   result = _op_def_lib.apply_op(\"Reshape\", tensor=tensor, shape=shape,\n\u001b[0;32m-> 2630\u001b[0;31m                                 name=name)\n\u001b[0m\u001b[1;32m   2631\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.pyc\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    502\u001b[0m             \u001b[0;31m# What type does convert_to_tensor think it has?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m             observed = ops.internal_convert_to_tensor(\n\u001b[0;32m--> 504\u001b[0;31m                 values, as_ref=input_arg.is_ref).dtype.name\n\u001b[0m\u001b[1;32m    505\u001b[0m             prefix = (\"Input '%s' of '%s' Op has type %s that does not match\" %\n\u001b[1;32m    506\u001b[0m                       (input_name, op_type_name, observed))\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype)\u001b[0m\n\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m           \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.pyc\u001b[0m in \u001b[0;36m_tensor_shape_tensor_conversion_function\u001b[0;34m(s, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    126\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_fully_defined\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     raise ValueError(\n\u001b[0;32m--> 128\u001b[0;31m         \"Cannot convert a partially known TensorShape to a Tensor: %s\" % s)\n\u001b[0m\u001b[1;32m    129\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot convert a partially known TensorShape to a Tensor: (?, 8, 8, 128)"
     ]
    }
   ],
   "source": [
    "for emo_class, emo_name in zip(combine, emotion):\n",
    "    print ('Class: %s training procedure' % (emo_name))\n",
    "    train_gan(emo_class, emo_name)\n",
    "print ('\\nEnd of training\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anger\n",
      "[[1 0 0 0 0]]\n",
      "knn_label: [[0 0 1 0 0]] , ts_label: [[0 0 0 1 0]]\n",
      "Discard\n",
      "[[1 0 0 0 0]]\n",
      "knn_label: [[0 0 1 0 0]] , ts_label: [[0 0 0 1 0]]\n",
      "Discard\n",
      "[[1 0 0 0 0]]\n",
      "knn_label: [[0 0 1 0 0]] , ts_label: [[0 0 0 1 0]]\n",
      "Discard\n",
      "[[1 0 0 0 0]]\n",
      "knn_label: [[0 0 1 0 0]] , ts_label: [[0 0 0 1 0]]\n",
      "Discard\n",
      "[[1 0 0 0 0]]\n",
      "knn_label: [[0 0 1 0 0]] , ts_label: [[0 0 0 1 0]]\n",
      "Discard\n",
      "[[1 0 0 0 0]]\n",
      "knn_label: [[0 0 1 0 0]] , ts_label: [[0 0 0 1 0]]\n",
      "Discard\n",
      "[[1 0 0 0 0]]\n",
      "knn_label: [[0 0 1 0 0]] , ts_label: [[0 0 0 1 0]]\n",
      "Discard\n",
      "[[1 0 0 0 0]]\n",
      "knn_label: [[0 0 1 0 0]] , ts_label: [[0 0 0 1 0]]\n",
      "Discard\n",
      "[[1 0 0 0 0]]\n",
      "knn_label: [[0 0 1 0 0]] , ts_label: [[0 0 0 1 0]]\n",
      "Discard\n",
      "[[1 0 0 0 0]]\n",
      "knn_label: [[0 0 1 0 0]] , ts_label: [[0 0 0 1 0]]\n",
      "Discard\n",
      "[[1 0 0 0 0]]\n",
      "knn_label: [[0 0 1 0 0]] , ts_label: [[0 0 0 1 0]]\n",
      "Discard\n",
      "[[1 0 0 0 0]]\n",
      "knn_label: [[0 0 1 0 0]] , ts_label: [[0 0 0 1 0]]\n",
      "Discard\n",
      "[[1 0 0 0 0]]\n",
      "knn_label: [[0 0 1 0 0]] , ts_label: [[0 0 0 1 0]]\n",
      "Discard\n",
      "[[1 0 0 0 0]]\n",
      "knn_label: [[0 0 1 0 0]] , ts_label: [[0 0 0 1 0]]\n",
      "Discard\n",
      "[[1 0 0 0 0]]\n",
      "knn_label: [[0 0 1 0 0]] , ts_label: [[0 0 0 1 0]]\n",
      "Discard\n",
      "[[1 0 0 0 0]]\n",
      "knn_label: [[0 0 1 0 0]] , ts_label: [[0 0 0 1 0]]\n",
      "Discard\n",
      "[[1 0 0 0 0]]\n",
      "knn_label: [[0 0 1 0 0]] , ts_label: [[0 0 0 1 0]]\n",
      "Discard\n",
      "[[1 0 0 0 0]]\n",
      "knn_label: [[0 0 1 0 0]] , ts_label: [[0 0 0 1 0]]\n",
      "Discard\n",
      "[[1 0 0 0 0]]\n",
      "knn_label: [[0 0 1 0 0]] , ts_label: [[0 0 0 1 0]]\n",
      "Discard\n",
      "[[1 0 0 0 0]]\n",
      "knn_label: [[0 0 1 0 0]] , ts_label: [[0 0 0 1 0]]\n",
      "Discard\n",
      "[[1 0 0 0 0]]\n",
      "knn_label: [[0 0 1 0 0]] , ts_label: [[0 0 0 1 0]]\n",
      "Discard\n",
      "[[1 0 0 0 0]]\n",
      "knn_label: [[0 0 1 0 0]] , ts_label: [[0 0 0 1 0]]\n",
      "Discard\n",
      "[[1 0 0 0 0]]\n",
      "knn_label: [[0 0 1 0 0]] , ts_label: [[0 0 0 1 0]]\n",
      "Discard\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-e02cbf1af359>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;31m# run the knn procedure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mknn_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mknn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mknn_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mknn_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;31m# use teacher-student model to classify this fake data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-b43890e64a2f>\u001b[0m in \u001b[0;36mknn\u001b[0;34m(target_data)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mdist_choice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'eu'\u001b[0m\u001b[0;31m# distance type (eu , manha , chebyshev)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist_choice\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'eu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meuclidean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist_choice\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'manha'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmanhattan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-b43890e64a2f>\u001b[0m in \u001b[0;36meuclidean\u001b[0;34m(train, test, train_num)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m# number of training data: i\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mdist_temp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist_temp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#generate_fake_data()\n",
    "add_data_amount = [0,0,0,0,0]\n",
    "for i in range(5):\n",
    "        fake_data = np.load('./fake_data/' + emotion[i] + '/' + \"fake_data.npy\")\n",
    "        gan_label = [[0,0,0,0,0]]\n",
    "        gan_label[0][i] += 1\n",
    "        gan_label = np.asarray(gan_label)\n",
    "        print (emotion[i])\n",
    "        for j in range(2000):\n",
    "            fake_data = reshape(fake_data, [-1,256])\n",
    "\n",
    "            # run the knn procedure\n",
    "            knn_label = knn(fake_data)\n",
    "            knn_label = np.asarray(knn_label)\n",
    "            # use teacher-student model to classify this fake data\n",
    "            ts_label = tea_stu_model(fake_data,train_label, False)\n",
    "            ts_label = np.asarray(ts_label)\n",
    "            # whether to add this fake data to the training data\n",
    "            print (gan_label)\n",
    "            print ('knn_label: {0} , ts_label: {1}'.format(knn_label, ts_label))\n",
    "            if np.array_equal(gan_label, knn_label) == True and np.array_equal(knn_label, ts_label) == False:\n",
    "                print ('Add')\n",
    "                train_data = np.concatenate((train_data, fake_data), axis = 0)\n",
    "                train_label = np.concatenate((train_label, knn_label), axis = 0)\n",
    "                add_data_amount[i] += 1\n",
    "            else:\n",
    "                print ('Discard')\n",
    "        print ('{0} extends {1} data.'.format(emotion[i], add_data_amount[i]))\n",
    "        \n",
    "np.savetxt('./extend_train_data/new_train_data.txt', train_data, delimiter=' ', fmt='%f')\n",
    "np.savetxt('./extend_train_data/new_train_data_label.txt', train_label, delimiter=' ', fmt='%d')\n",
    "\n",
    "new_train_data, new_train_label = read_fake_data('new_train_data.txt', 'new_train_data_label.txt')\n",
    "tea_stu_model(new_train_data, new_train_label, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
