{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import time\n",
    "import curses\n",
    "from pylab import *\n",
    "import tflearn as tl\n",
    "from libs.utils import weight_variable, bias_variable\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readFile(filename, isSP=False) :\n",
    "    f = open(filename, 'r')\n",
    "    \n",
    "    # read header\n",
    "    line = f.readline()\n",
    "    while line != '@data\\n' :\n",
    "        line = f.readline()\n",
    "    \n",
    "    line = f.readline()  #空白行\n",
    "    \n",
    "    #read data\n",
    "    data = []\n",
    "    label = []\n",
    "    size = [0, 0, 0, 0, 0]\n",
    "    \n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if line == '' :  # EOF\n",
    "            break\n",
    "        \n",
    "        #data processing\n",
    "        temp = []\n",
    "        for numstr in line.split(\",\") :\n",
    "            if numstr :\n",
    "                try :\n",
    "                    numFl = float(numstr)\n",
    "                    temp.append(numFl)\n",
    "                except ValueError as e :\n",
    "                    \n",
    "                    if not isSP :\n",
    "                        numstr = numstr[:-1]  #skip '\\n'\n",
    "                    else :\n",
    "                        numstr = numstr[1]  #get class (supervector only)\n",
    "                    \n",
    "                    if numstr == 'Anger' or numstr == 'A' :##標為0\n",
    "                        L = [1,0,0,0,0]\n",
    "                        size[0] += 1\n",
    "                    if numstr == 'Emphatic' or numstr == 'E' :##標為1\n",
    "                        L = [0,1,0,0,0]\n",
    "                        size[1] += 1\n",
    "                    if numstr == 'Neutral' or numstr == 'N' :##標為2\n",
    "                        L = [0,0,1,0,0]\n",
    "                        size[2] += 1\n",
    "                    if numstr == 'Positive' or numstr == 'P' :##標為3\n",
    "                        L = [0,0,0,1,0]\n",
    "                        size[3] += 1\n",
    "                    if numstr == 'Rest' or numstr == 'R' :##標為4\n",
    "                        L = [0,0,0,0,1]\n",
    "                        size[4] += 1\n",
    "                    \n",
    "                    label.append(L)  # get label\n",
    "        data.append(temp)  # get data\n",
    "    \n",
    "    f.close()\n",
    "    \n",
    "    data = np.asarray(data)\n",
    "    label = np.asarray(label)\n",
    "    #data, label = randomize(data, label)  #from Homework1 (Udacity)\n",
    "    \n",
    "    \n",
    "    return data, label, size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def separa_fau(data,label):\n",
    "    a=[]\n",
    "    e=[]\n",
    "    n=[]\n",
    "    p=[]\n",
    "    r=[]\n",
    "    for i in range(label.shape[0]):\n",
    "        if((np.argmax(label[i]))==0):a.append(data[i])\n",
    "        if((np.argmax(label[i]))==1):e.append(data[i])\n",
    "        if((np.argmax(label[i]))==2):n.append(data[i])\n",
    "        if((np.argmax(label[i]))==3):p.append(data[i])\n",
    "        if((np.argmax(label[i]))==4):r.append(data[i])\n",
    "    a=np.asarray(a)\n",
    "    e=np.asarray(e)\n",
    "    n=np.asarray(n)\n",
    "    p=np.asarray(p)\n",
    "    r=np.asarray(r)\n",
    "    return a,e,n,p,r            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\xe8\\xa8\\x93\\xe7\\xb7\\xb4\\xe8\\xb3\\x87\\xe6\\x96\\x99\\xef\\xbc\\x9a', (9959, 384), (9959, 5), [881, 2093, 5590, 674, 721])\n",
      "('\\xe6\\xb8\\xac\\xe8\\xa9\\xa6\\xe8\\xb3\\x87\\xe6\\x96\\x99\\xef\\xbc\\x9a', (8257, 384), (8257, 5))\n"
     ]
    }
   ],
   "source": [
    "filename_train = './data/fau_train_nor.arff'\n",
    "filename_test =  './data/fau_test_nor.arff'\n",
    "train_data, train_label, train_size = readFile(filename_train)\n",
    "test_data, test_label, test_size = readFile(filename_test)\n",
    "print(\"訓練資料：\",train_data.shape,train_label.shape,train_size)\n",
    "print(\"測試資料：\",test_data.shape,test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((881, 384), (2093, 384), (5590, 384), (674, 384), (721, 384))\n"
     ]
    }
   ],
   "source": [
    "fau_a,fau_e,fau_n,fau_p,fau_r=separa_fau(train_data,train_label)\n",
    "print(fau_a.shape,fau_e.shape,fau_n.shape,fau_p.shape,fau_r.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_d = tf.placeholder(tf.float32, shape = [None, 384])\n",
    "x_g = tf.placeholder(tf.float32, shape = [None, 128])\n",
    "\n",
    "d_layer1=64\n",
    "d_layer2=1\n",
    "\n",
    "g_dim = 128\n",
    "g_layer1=256\n",
    "g_layer2=384\n",
    "\n",
    "\n",
    "weights = {\n",
    "    \"w_d1\" : weight_variable([384, d_layer1], \"w_d1\"),\n",
    "    \"w_d2\" : weight_variable([d_layer1, d_layer2], \"w_d2\"),\n",
    "    #\"w_d3\" : weight_variable([d_layer2, 1], \"w_d3\"),\n",
    "\n",
    "\n",
    "    \"w_g1\" : weight_variable([g_dim, g_layer1], \"w_g1\"),\n",
    "    \"w_g2\" : weight_variable([g_layer1, g_layer2], \"w_g2\"),\n",
    "    #\"w_g3\" : weight_variable([g_layer2, 384], \"w_g3\")\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    \"b_d1\" : bias_variable([d_layer1], \"b_d1\"),\n",
    "    \"b_d2\" : bias_variable([d_layer2], \"b_d2\"),\n",
    "    #\"b_d3\" : bias_variable([1], \"b_d3\"),\n",
    "\n",
    "    \"b_g1\" : bias_variable([g_layer1], \"b_g1\"),\n",
    "    \"b_g2\" : bias_variable([g_layer2], \"b_g2\"),\n",
    "    #\"b_g3\" : bias_variable([384], \"b_g3\")\n",
    "}\n",
    "\n",
    "#var_d = [weights[\"w_d1\"], weights[\"w_d2\"], weights[\"w_d3\"], biases[\"b_d1\"], biases[\"b_d2\"], biases[\"b_d3\"]]\n",
    "#var_g = [weights[\"w_g1\"], weights[\"w_g2\"], weights[\"w_g3\"], biases[\"b_g1\"], biases[\"b_g2\"], biases[\"b_g3\"]]\n",
    "\n",
    "var_d = [weights[\"w_d1\"], weights[\"w_d2\"],biases[\"b_d1\"], biases[\"b_d2\"]]\n",
    "var_g = [weights[\"w_g1\"], weights[\"w_g2\"],biases[\"b_g1\"], biases[\"b_g2\"]]\n",
    "\n",
    "def generator(z):\n",
    "    h_g1 = tf.nn.relu(tf.add(tf.matmul(z, weights[\"w_g1\"]), biases[\"b_g1\"]))\n",
    "    h_g2 = tf.nn.sigmoid(tf.add(tf.matmul(h_g1, weights[\"w_g2\"]),biases[\"b_g2\"]))\n",
    "    #h_g3 = tf.nn.sigmoid(tf.add(tf.matmul(h_g2, weights[\"w_g3\"]),biases[\"b_g3\"]))\n",
    "    return h_g2\n",
    "\n",
    "\n",
    "def discriminator(x):\n",
    "    h_d1 = tf.nn.relu(tf.add(tf.matmul(x, weights[\"w_d1\"]), biases[\"b_d1\"]))\n",
    "    h_d2 = tf.nn.sigmoid(tf.add(tf.matmul(h_d1, weights[\"w_d2\"]), biases[\"b_d2\"]))\n",
    "    #h_d3 = tf.nn.sigmoid(tf.add(tf.matmul(h_d2, weights[\"w_d3\"]), biases[\"b_d3\"]))\n",
    "    return h_d2\n",
    "\n",
    "def sample_Z(m, n):\n",
    "    return np.random.uniform(-1., 1., size=[m, n])\n",
    "\n",
    "g_sample = generator(x_g)\n",
    "d_real= discriminator(x_d)\n",
    "d_fake = discriminator(g_sample)\n",
    "\n",
    "d_loss = -tf.reduce_mean(tf.log(d_real) + tf.log(1. - d_fake))#希望d_real是1,d_fake是0.加負號後loss最小值為0  log(0.5)=-0.7\n",
    "g_loss = -tf.reduce_mean(tf.log(d_fake))#希望d_fake是1,loss最小值為0\n",
    "\n",
    "global_step = tf.Variable(0)\n",
    "# 只更新 discriminator\n",
    "learning_rate_d = tf.train.exponential_decay(0.0005,decay_steps=1000,decay_rate=0.8,global_step=global_step)\n",
    "d_optimizer = tf.train.AdamOptimizer(learning_rate_d).minimize(d_loss, var_list= var_d)\n",
    "#d_optimizer=tf.train.GradientDescentOptimizer(learning_rate_d, use_locking=False).minimize(d_loss, var_list= var_d)\n",
    "\n",
    "# 只更新 generator parameters\n",
    "learning_rate_g = tf.train.exponential_decay(0.0001,decay_steps=1000, decay_rate=0.8,global_step=global_step)\n",
    "g_optimizer = tf.train.AdamOptimizer(learning_rate_g).minimize(g_loss, var_list= var_g)\n",
    "#g_optimizer=tf.train.GradientDescentOptimizer(learning_rate_g, use_locking=False).minimize(g_loss,var_list= var_g)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot(num,samples):\n",
    "    samples=np.concatenate((samples,np.zeros((samples.shape[0],16))),axis=1)\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(20, 20), cmap = 'gray')\n",
    "\n",
    "    plt.savefig('./pic/figure %d .png'%(num))\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-b814664ba611>:5 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "step 0, discriminator loss 1.35310  generator loss 1.03978\n",
      "INFO:tensorflow:./mod/model1.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "step 1000, discriminator loss 0.57162  generator loss 1.56867\n",
      "step 2000, discriminator loss 1.47695  generator loss 0.75193\n",
      "step 3000, discriminator loss 1.20445  generator loss 0.92002\n",
      "step 4000, discriminator loss 1.10102  generator loss 0.84628\n",
      "INFO:tensorflow:./mod/model2.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "step 5000, discriminator loss 0.94027  generator loss 1.16438\n",
      "step 6000, discriminator loss 1.27745  generator loss 0.89141\n",
      "step 7000, discriminator loss 0.64084  generator loss 1.40714\n",
      "step 8000, discriminator loss 1.08672  generator loss 1.03241\n",
      "INFO:tensorflow:./mod/model3.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "step 9000, discriminator loss 1.11010  generator loss 0.93780\n",
      "step 10000, discriminator loss 0.83504  generator loss 1.17516\n"
     ]
    }
   ],
   "source": [
    "num=1\n",
    "batch_size = 100\n",
    "global_step=10001\n",
    "sess = tf.InteractiveSession()\n",
    "init_op = tf.initialize_all_variables()\n",
    "saver = tf.train.Saver(max_to_keep=0)\n",
    "sess.run(init_op)\n",
    "train_data=fau_n\n",
    "for step in range(global_step):\n",
    "    offset = (step * batch_size) % (train_data.shape[0] - batch_size)\n",
    "    batch_x = train_data[offset:(offset + batch_size), :]\n",
    "    _, d_loss_train = sess.run([d_optimizer, d_loss], feed_dict = {x_d: batch_x, x_g: sample_Z(batch_size, g_dim)})\n",
    "    _, g_loss_train = sess.run([g_optimizer, g_loss], feed_dict = {x_g: sample_Z(batch_size, g_dim)})\n",
    "    if step % 1000 == 0:\n",
    "            print(\"step %d, discriminator loss %.5f\" % (step, d_loss_train)),\n",
    "            print(\" generator loss %.5f\" % (g_loss_train))\n",
    "    if step % 4000 == 0: \n",
    "            g_sample_plot = g_sample.eval(feed_dict = {x_g: sample_Z(16, g_dim)})\n",
    "            plot(num,g_sample_plot)\n",
    "            save_path = saver.save(sess, \"./mod/model%d.ckpt\"%(num))\n",
    "            num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(1111,fau_r[0:16,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "init_op = tf.initialize_all_variables()\n",
    "saver = tf.train.Saver(max_to_keep=0)\n",
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.6094379"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=tf.log(0.2)\n",
    "sess.run(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
